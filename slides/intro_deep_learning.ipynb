{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un'introduzione al\n",
    "\n",
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cos'è il machine learning (o apprendimento automatico)?\n",
    "\n",
    "- Il machine learning (ML) è un insieme di tecniche che utilizza algoritmi per scoprire modelli o informazioni sui dati.\n",
    "- Caratteristica principale del machine learning è la capacità di imparare a fare previsioni dai dati senza essere esplicitamente programmati.\n",
    "- Fa parte del più ampio campo dell'intelligenza artificiale.\n",
    "- Ci sono molte tecniche diverse di machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Esempi di machine learning\n",
    "\n",
    "- Il più semplice algoritmo ML potrebbe essere le regressione lineare.\n",
    "- Una tecnica più avanzata è il *K-means clustering*. È un modo per trovare cluster di punti nei vostri dati senza dover inserire alcuna etichetta esplicita.\n",
    "- La tecnica più famosa sono le *reti neurali* (NN), che sono ispirate al cervello e utilizzano una rete direzionata di neuroni per descrivere le caratteristiche di un dataset.\n",
    "    - Nell'ultimo decennio si è reso possibile l'utilizzo delle *reti neurali profonde* (Deep Neural Networks) che permettono di apprendere modelli sempre più sofisticati, dando inizio al moderno battage del *Deep Learning*.\n",
    "\n",
    "<img src=\"dl-timeline.png\" alt=\"Deep learning timeline\" style=\"width: 700px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cosa sono le reti neurali\n",
    "\n",
    "Le reti neurali sono un insieme di neuroni artificiali collegati tra loro, il cui funzionamento è ispirato a quello dei neuroni che formano il cervello.\n",
    "\n",
    "Semplificando enormemente, un neurone in natura riceve degli stimoli elettrici da altri neuroni tramite le dendriti (entrate), e se stimolato \"abbastanza\" emette a sua volta una scarica elettrica attraverso il suo assone (uscita). Lo stimolo in uscita è a sua volta inviato ad altri neuroni.  Questa struttura e il modo in cui in neuroni sono disposti in rete (a strati) è l'ispirazione diretta per le reti neurali artificiali (Artificial Neural Networks - ANN).\n",
    "\n",
    "| Un Neurone | Corteccia cerebrale |\n",
    "|------|------|\n",
    "|<img src=\"neuron_nature.png\" alt=\"Un neurone\" style=\"height: 280px; margin:0;\"/>| <img src=\"cortex.png\" alt=\"Corteccia cerebrale\" style=\"height: 280px; margin:0;\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neuroni Artificiali: il perceptron\n",
    "Un neurone artificiale (perceptron) funziona in maniera simile: ha più ingressi e può passare il suo valore di uscita a più neuroni.\n",
    "- Dal punto di vista matematico si tratta di una funzione creata per composizione di un polinomio di primo grado e una funzione non lineare (funzione di attivazione).\n",
    "\n",
    "Un neurone calcola il suo valore di uscita tramite una somma pesata dei valori in entrata, $z = \\sum_i{x_iw_i}+b$ dove $x_i$ è il valore di ingresso, $w_i$ è un peso assegnato a quella connessione in ingresso e $b$ è un bias (peso libero, che potremo anche indicare come $w_0$). Questo $z$ viene poi passato attraverso una qualche *funzione di attivazione* per determinare l'uscita del neurone.\n",
    "\n",
    "<img src=\"neuron.png\" alt=\"An artificial neuron\" style=\"width: 800px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Funzioni di attivazione\n",
    "Le funzioni di attivazione hanno un duplice scopo:\n",
    "- permettere al neurone di esibire un comportamento non lineare (e quindi modellare comportamenti complessi).\n",
    "- riportare i valori della sommatoria all'interno di un intervallo di valori desiderato.\n",
    "\n",
    "Qualche esempio di funzioni di attivazione tipicamente usate:\n",
    "\n",
    "<img src=\"activation_functions.png\" alt=\"An artificial neuron\" style=\"width: 600px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Network (ANN)\n",
    "\n",
    "Le ANN sono composte da reti di neuroni. Gli input a ciascun neurone provengono o dagli output di altri neuroni o sono input espliciti dell'utente. Questo semplice meccanismo permette di creare reti di neuroni anche molto grandi e complesse.\n",
    "\n",
    "Similmente all'ogranizzazione dei neuroni nella corteccia, le reti neurali artificiali sono spesso organizzate in strati (layer) che raggruppano tutti i neuroni allo stesso livello di profondità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cos'è il Deep Learning?\n",
    "\n",
    "Il deep learning (letteralmente \"apprendimento profondo\") comporta l'apprendimento per strati, che permettono ad un computer di costruire una gerarchia di concetti complessi a partire da concetti più semplici. Le reti neurali multistrato, o profonde (Deep Neural Network - DNN), sono lo strumento principe utilizzato per il deep learning. \n",
    "\n",
    "<img src=\"network.png\" alt=\"Una rete neurale artificiale\" style=\"width: 400px; margin:0 auto;\"/>\n",
    "\n",
    "In questa rete ogni neurone di uno strato (layer) è collegato ad ogni neurone di quello successivo. Ad ogni freccia del diagramma è assegnato un peso ($w$).\n",
    "\n",
    "Si immettono valori sul lato sinistro della rete e i dati passano attraverso la rete da uno strato all'altro fino a quando non si ottiene il valore dallo strato di uscita (struttura feed-forward, senza collegamenti all'indietro). I layer intermedi sono chiamati \"hidden layers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Il Deep learning dal punto di vista computazionale\n",
    "Sia i dati di input che i pesi appartenenti a una rete possono essere rappresentati sotto forma di tensori (matrici multidimensionali) e le operazioni necessarie per i calcoli sulla rete possono essere espressi in funzione di calcoli matriciali.\n",
    "\n",
    "| Notazione standard | Calcolo matriciale |\n",
    "|------|------|\n",
    "|<img src=\"multilayer_perceptron.png\" alt=\"Rete e calcoli\" style=\"width: 500px; margin:0 auto;\"/>| <img src=\"multilayer_perceptron_notation.png\" alt=\"Corteccia cerebrale\" style=\"width: 400px; margin:0;\"/>|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Le DNN sono approssimatori universali (dimostrato nel 1989!)\n",
    "\n",
    "È dimostrato che una rete feed-forward con un singolo hidden layer contenente un numero finito di neuroni approssima le funzioni continue.\n",
    " - La capacità migliora esponenzialmente all'aumentare della profondità della rete.\n",
    " - Moltissimi fenomeni in natura sono rappresentabili (o approssimabili) tramite composizione di funzioni continue.\n",
    "\n",
    "<img src=\"universal_theorem.png\" alt=\"An artificial neural network\" style=\"width: 600px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quindi perchè solo ora? Piccolo excursus storico\n",
    "La teoria sottostante alle DNN era nota già 30 e più anni fa. I primi esprimenti risalgono agli anni 50.\n",
    "\n",
    "| Il perceptron - Rosenblatt (1957)|\n",
    "|------|\n",
    "|<img src=\"perceptron.jpg\" alt=\"Il perceptron (1957) del prof. Rosenblatt\" style=\"width: 600px; margin:0 auto;\"/>|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cosa è cambiato nell'ultimo decennio?\n",
    " - Sono state trovate tecniche per migliorare la stabilità numerica delle reti.\n",
    " - L'avvento del calcolo su GPU (schede grafiche) ha permesso di sfruttare il calcolo matriciale parallelo per creare reti con milioni di neuroni.\n",
    " - La disponibilità di grandi quantità di dati (l'era dei \"Big Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"timeline.png\" alt=\"Deep Learning Timeline\" style=\"heigth: 300px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cosa sono in grado di fare le moderne DNN? Un esempio\n",
    "Classificare il contenuto di una fotografia:\n",
    "\n",
    "<img src=\"dnn.png\" alt=\"An artificial neural network\" style=\"width: 800px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Come si calcolano i pesi?\n",
    "Il calcolo dei pesi in una rete viene effettuato attraverso un processo di *training* (allenamento). Questo si traduce in utilizzare molti esempi di dati per elaborare iterativamente buoni valori per i pesi della rete.\n",
    "Si può dire che le DNN \"imparano\" per esempi.\n",
    "\n",
    "## Perchè si usa questo approccio?\n",
    "I pesi di una DNN sono altamente interdipendenti. La modifica del peso di un neurone in un layer, avrà impatto non solo per i neuroni a cui si propaga direttamente, ma anche su tutti i neuroni dei layer successivi, fino all'uscita. Per questo motivo, sappiamo che non possiamo ottenere il miglior set di pesi ottimizzandone uno alla volta; va cercato l'intero spazio delle possibili combinazioni di pesi contemporaneamente. L'idea è quella di modificare gradualmente tutti i pesi fino a raggiungere la combinazione ideale.\n",
    "\n",
    "<img src=\"connection_tweak.png\" alt=\"Connesione di un peso\" style=\"width: 300px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training: cosa serve per l'allenamento\n",
    "Per addestrare una rete è necessario definire:\n",
    " - Un set di dati di addestramento (training set) con eventuali label\n",
    " - Un set di dati di validazione (validation e/o test set) con eventuali label\n",
    " - Un insieme di pesi iniziali\n",
    " - Una funzione per il calcolo dell'errore (loss function) per determinare quanto bene la rete sta eseguendo il compito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# I dataset di training e test (o validation)\n",
    "\n",
    "Sono necessari due set di dati. Uno sarà utilizzato per allenare la rete e l'altro servirà per testarne le capacità predittive.\n",
    "\n",
    "È importante che questi set di dati siano disgiunti per evitare un *overfitting* (ovvero che la rete impari a memoria).\n",
    "\n",
    "È normale iniziare con un unico set di dati che si vuole conoscere e dividerlo in un set training dell'80% e un set di dati di test del 20%.\n",
    "<img src=\"train_test_validation.png\" alt=\"Connesione di un peso\" style=\"width: 500px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training: come funziona\n",
    "Il metodo con cui vengono addestrate le DNN è una tecnica chiamata *the backward propogation of the error* (retropropagazione dell'errore), abbreviata spesso in *backpropagation*. Il ciclo completo di training prevede i seguenti passaggi:\n",
    "                                                                                      \n",
    "1. Inizializzazione dei pesi dell rete con valori casuali (*random initialization*).\n",
    "2. Propagazione in avanti (*forward pass*)\n",
    "    * Inserimento del dato nella rete e genrazione dell'output\n",
    "    \n",
    "3. Calcolo dell'errore (*loss computation*) \n",
    "    * Valore di loss ~= (output desiderato) - (output ottenuto)\n",
    "    \n",
    "4. Propagazione all'indietro (*backpropogation*)\n",
    "    * Calcolo dei gradienti tramite la propagazione all'indietro dell'errore dall'uscita all'ingresso\n",
    "    \n",
    "5. Aggiornamento dei pesi della rete sulla base del gradiente (*optimization step*)\n",
    "6. Se vi sono ancora dati, ritorno al punto 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss functions\n",
    "La loss function prende in input l'output generato dalla DNN e l'ouput atteso e ritorna valore reale scalare chiamato *loss*. È la funzione che guida il processo di allenamento della DNN, e risponde alla domanda \"quanto il valore generato dalla DNN sull'input si discosta dal valore desiderato?\". Più basso è il valore di loss, migliore è la previsione del modello.\n",
    "\n",
    "Vi sono varie loss function che posso essere usate a seconda del problema che si sta affrontando.\n",
    "Le più usate per i problemi di regressione sono l'errore quadratico medio (Mean Squared Error - MSE, detta anche $l_2$ loss) e l'errore medio assoluto (Mean Absolute Error - MAE, detto anche $l_1$ loss), mentre per i problemi di classificazione è utilizzata molto spesso una qualche variante della cross-entropy.\n",
    "\n",
    "Si può usare qualsiasi funzione come loss, **a patto che sia derivalbile**.\n",
    "\n",
    "Da un altro punto di vista, si può vedere la loss come la funzione che prende in input tutti i paramentri della DNN (input, pesi) e ritorna un valore tanto maggiore, quanto più i parametri sono \"sbagliati\". L'obiettivo del processo di training è quello di trovare il set di *pesi* che minimizza la funzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"backprop1.png\" alt=\"Back propogation example\" style=\"width: 100%; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropogation (\"the backward propagation of errors\")\n",
    "\n",
    "Per capire in quale direzione il valore di loss scende più velocemente, è necessario calcolare la pendenza della loss function rispetto a tutti i pesi. Un gradiente è una generalizzazione multidimensionale di una derivata; è un vettore che contiene ciascuna delle derivate parziali della funzione rispetto a ciascuna variabile. In altre parole, è un vettore che contiene la pendenza della loss function lungo ogni asse.\n",
    "\n",
    "La backpropagation è una tecnica per il calcolo efficente dei gradienti nelle neural network e sfrutta il fatto che le DNN sono formate da una concatenazione di funzioni derivabili. La backpropagation opera partendo dal valore di uscita e applica la chain rule risalendo all'indietro attraverso i layer della DNN.\n",
    "\n",
    "Al termine del passo di backpropagation il computer ha calcolato tutti i gradienti per tutti i parametri della netowrk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization step\n",
    "\n",
    "Il passo di ottimizzazione (optimization step) utilizza i gradienti per modificare i pesi della DNN nella direzione che minimizza l'errore. Se il gradiente è positivo, il valore del peso dovrà essere diminuito, se negativo aumentato. Questa operazione è chiamata *discesa lungo il gradiente*.\n",
    "\n",
    "Vi sono varie strategie che possono essere usate per la modifica dei pesi. Quasi tutti gli algoritmi usano un passo base detto *learning rate* che viene moltiplicato per il valore del gradiente calcolato per il parmetro e sotratto al peso originale.\n",
    "\n",
    "Ciò significa che quanto più i pesi sono \"sbagliati\", tanto più vengono corretti. Quando questo processo rallenta, dopo molti esempi, si dice che la rete *converge*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Esempi di ottimizzazione ||\n",
    "|------|------|\n",
    "|<img src=\"optim1.gif\" alt=\"Rete e calcoli\" style=\"width: 450px; margin:0 auto;\"/>| <img src=\"optim2.gif\" alt=\"Corteccia cerebrale\" style=\"width: 450px; margin:0;\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Che struttura dovrebbe avere la rete?\n",
    "\n",
    "C'è un po' di arte e un po' di scienza per decidere l'architettura di una rete. Ci sono delle regole empiriche, e la letteratura negli anni ha trovato strategie che funzionano bene per determinate classi di problemi e dati, ma questa è una delle cose che bisogna sperimentare ed è naturalmente un campo attivo di ricerca.\n",
    "\n",
    "Molto dipende ovviamente dal dato utilizzato e dal fenomeno che si vuole modellare.\n",
    "\n",
    "In generale, il numero di hidden layer si riferisce al livello di astrazione che si sta osservando. Generalmente, problemi più complessi richiedono più hidden layers (cioè reti più profonde), ma questo ne rende a sua volta più difficile e oneroso il training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Neural network zoo |\n",
    "|------|\n",
    "|<img src=\"nn_zoo.png\" alt=\"Neural network zoo\" style=\"width: 500px; margin:0;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Giochiamo un poco: https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CONVOLUTIONAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduzione breve all'analisi delle immagini\n",
    "\n",
    "L'esempio su playground ha funzionato bene, ma il grande svantaggio è che le feature usate erano sintetiche o già elaborate da un dataset. Vediamo come usare le capacità dell DNN su dati un po' più greezzi: le immagini.\n",
    "\n",
    "Ci sono stati molti progressi nell'analisi delle immagini, ma al centro della maggior parte di esse c'è la *convoluzione del kernel*. Questo inizia trattando l'immagine come una griglia di numeri, dove ogni numero rappresenta la luminosità del pixel\n",
    "\n",
    "$$\n",
    "\\begin{matrix} \n",
    "105 & 102 & 100 & 97 & 96 & \\dots \\\\\n",
    "103 & 99 & 103 & 101 & 102 & \\dots \\\\\n",
    "101 & 98 & 104 & 102 & 100 & \\dots \\\\\n",
    "99 & 101 & 106 & 104 & 99 & \\dots \\\\\n",
    "104 & 104 & 104 & 100 & 98 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{matrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definiamo un kernel\n",
    "\n",
    "Si può quindi creare un *kernel* che definisce un filtro da applicare all'immagine:\n",
    "\n",
    "$$\n",
    "Kernel = \\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A seconda dei valori del kernel, verranno eseguite diverse operazioni di filtraggio. Le più comuni sono:\n",
    "\n",
    " - affilare (mostrato sopra)\n",
    " - sfocatura\n",
    " - rilevamento dei bordi (direzionale o isotropo)\n",
    " \n",
    "I valori dei kernel sono creati dall'analisi matematica e sono generalmente fissi. Potete vedere alcuni esempi nella pagina [Wikipedia sui kernel](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applying a kernel\n",
    "\n",
    "This kernel is then overlaid over each set of pizels in the image, corresponding values are multiplied and then the total is summed:\n",
    "\n",
    "<img src=\"conv1.jpg\" alt=\"Convolution\" style=\"width: 600px; margin:0 auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Primo pixel\n",
    "\n",
    "![Convolution](conv3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Secondo pixel\n",
    "\n",
    "![Convolution](conv4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gestire i bordi\n",
    "\n",
    "![Convolution](conv5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prima e dopo\n",
    "\n",
    "Se si utilizza un kernel di rilevamento dei bordi (Sobel), si vedrà il seguente effetto\n",
    "\n",
    "![Before and after](filter.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "Il cuore delle reti neurali convoluzionali (CNN) è la loro capacità di creare automaticamente kernel che rilevano caratteristiche salienti dell'immagine. Se attentamente combinate, è possibile creare una rete che ha strati di astrazione che vanno da \"c'è un bordo qui\" a \"c'è un occhio qui\" a \"è questa una persona\".\n",
    "\n",
    "Dal punto di vista della rete neurale, c'è poco di diverso nel processo di training. Si può semplicemente trattare ogni elemento del kernel di convoluzione come un peso, come abbiamo fatto prima. L'algoritmo di backpropogation imparerà automaticamente i valori corretti per descrivere il set di dati dell'addestramento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| DNN standard | CNN |\n",
    "|------|------|\n",
    "|<img src=\"neural_net2.jpeg\" alt=\"Rete e calcoli\" style=\"width: 400px; margin:0 auto;\"/>| <img src=\"cnn.jpeg\" alt=\"Corteccia cerebrale\" style=\"width: 400px; margin:0;\"/>|\n",
    "\n",
    "Una ConvNet dispone i suoi neuroni in tre dimensioni (larghezza, altezza, profondità), come visualizzati in uno dei layer. Ogni layer di una ConvNet trasforma il volume di ingresso 3D in un volume di uscita 3D. In questo esempio, il layer di ingresso rosso contiene l'immagine, quindi la sua larghezza e l'altezza sarebbero le dimensioni dell'immagine, e la profondità sarebbe di 3 (canali rosso, verde, blu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Typical CNN\n",
    "\n",
    "![Typical CNN](typical_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image segmentation\n",
    "\n",
    "![Mapillary](mapillary.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Riconoscimento della scrittura\n",
    "\n",
    "Il set di dati MNIST è una raccolta di 70.000 immagini di 28×28 pixel di cifre scansionate e scritte a mano.\n",
    "\n",
    "![MNIST examples](MnistExamples.png)\n",
    "\n",
    "Vogliamo creare una rete che possa, data un'immagine simile di una cifra, identificarne il valore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "* Una rete neurale ricorrente (recurrent neural network, RNN) è una classe di rete neurale artificiale in cui i valori di uscita di uno strato di un livello superiore vengono utilizzati come ingresso ad uno strato di livello inferiore.\n",
    "\n",
    "* Quest'interconnessione tra strati permette l'utilizzo di uno degli strati come memoria di stato, e consente, fornendo in ingresso una sequenza temporale di valori, di modellarne un comportamento dinamico temporale dipendente dalle informazioni ricevute agli istanti di tempo precedenti.\n",
    "\n",
    "* Ciò le rende applicabili a compiti di analisi predittiva su sequenze di dati, quali possono essere ad esempio il riconoscimento della grafia o il riconoscimento vocale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"RNN-unrolled.png\" alt=\"\" style=\"width: 800px; margin:0;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"lstm.png\" alt=\"\" style=\"width: 600px; margin:0;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine (per ora)\n",
    "### franch@fbk.eu\n",
    "\n",
    "Credits:\n",
    "\n",
    "- Dog photo: CC BY 2.0 [Emily Mathews](https://www.flickr.com/photos/eamathe/14517807267/)\n",
    "- Irises: <a href=\"https://commons.wikimedia.org/w/index.php?curid=170298\">Iris setosa</a> (by\n",
    "<a href=\"https://commons.wikimedia.org/wiki/User:Radomil\">Radomil</a>, CC BY-SA 3.0),\n",
    "<a href=\"https://commons.wikimedia.org/w/index.php?curid=248095\">Iris versicolor</a> (by\n",
    "<a href=\"https://commons.wikimedia.org/wiki/User:Dlanglois\">Dlanglois</a>, CC BY-SA 3.0),\n",
    "and <a href=\"https://www.flickr.com/photos/33397993@N05/3352169862\">Iris virginica</a>\n",
    "(by <a href=\"https://www.flickr.com/photos/33397993@N05\">Frank Mayfield</a>, CC BY-SA\n",
    "2.0).\n",
    "- Neuron image: CC BY 3.0 [BruceBlaus](https://commons.wikimedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png)\n",
    "- Artificial Neuron image: CC BY-SA 3.0 [Chrislb](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png)\n",
    "- Cortex image: Public Domain [Santiago Ramon y Cajal](https://commons.wikimedia.org/w/index.php?curid=8513016)\n",
    "- Kernel convolution images: http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html\n",
    "- CNN layout: CC BY-SA 4.0 [Aphex34](https://commons.wikimedia.org/wiki/File:Typical_cnn.png)\n",
    "- XKCD Compiling comic: CC BY-NC 2.5 [Randall Munroe](https://xkcd.com)\n",
    "- Image segmentation: Copyright [Mapillary](https://blog.mapillary.com/update/2018/01/11/new-benchmarks-for-semantic-segmentation-models.html)\n",
    "- Deep painterly: [Fujun Luan](https://github.com/luanfujun/deep-painterly-harmonization)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
