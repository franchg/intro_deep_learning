{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One paramount important requirement in DL model training and learning is the ability to store and **save** the internal state of a model for the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to **load and save** a model for:\n",
    "\n",
    "1. **inference**; \n",
    "2. **re-start** the training where we left (i.e. _checkpoint_ )\n",
    "3. **save** the best hyper-parameter configuration in a randomised _grid search_ optimisation\n",
    "4. $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **two** approaches for saving and loading models for inference in PyTorch. \n",
    "\n",
    "The **first** is saving and loading the `state_dict`, and the second is saving and loading the **entire model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's define our (usual) model and optimiser first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = torch.relu(self.linear1(x))\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the entire model using `pickle`\n",
    "\n",
    "We could use the Python `pickle` module to save and load an entire model.\n",
    "\n",
    "Using this approach yields the most intuitive syntax and involves the least amount of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriele/Documents/fbk/miarad/talk/intro_deep_learning/tutorial_pytorch/.venv/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_serialisation.pkl', 'wb') as pkf: \n",
    "    pickle.dump(model, pkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight        torch.Size([100, 1000]) 100000\n",
      "linear1.bias          torch.Size([100])   100\n",
      "linear2.weight        torch.Size([10, 100]) 1000\n",
      "linear2.bias          torch.Size([10])    10\n"
     ]
    }
   ],
   "source": [
    "with open('model_serialisation.pkl', 'rb') as pkf:\n",
    "    model_pkl = pickle.load(pkf)\n",
    "    for name_str, param in model_pkl.named_parameters():\n",
    "        print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However**, this method is far from being flexible: the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. \n",
    "\n",
    "The reason for this is because pickle does not save the model class itself. \n",
    "Rather, it saves a path to the file containing the class, which is used during load time. \n",
    "\n",
    "**For this reason**, your code can break in various ways when used in other projects or after refactors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `model|optim.state_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the learnable parameters (i.e. weights and biases) of a `torch.nn.Module` model are contained in the model’s parameters (accessed with `model.parameters()`). \n",
    "\n",
    "A `state_dict` is simply a Python dictionary object that maps each layer to its parameter tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight        torch.Size([100, 1000]) 100000\n",
      "linear1.bias          torch.Size([100])   100\n",
      "linear2.weight        torch.Size([10, 100]) 1000\n",
      "linear2.bias          torch.Size([10])    10\n"
     ]
    }
   ],
   "source": [
    "# model (named) parameters\n",
    "for name_str, param in model.named_parameters():\n",
    "    print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-0.0132, -0.0153, -0.0047,  ...,  0.0122, -0.0004,  0.0304],\n",
       "           [-0.0260, -0.0314, -0.0120,  ...,  0.0271,  0.0107, -0.0053],\n",
       "           [-0.0219, -0.0167,  0.0007,  ..., -0.0293,  0.0283,  0.0269],\n",
       "           ...,\n",
       "           [-0.0116, -0.0082,  0.0290,  ..., -0.0251, -0.0084, -0.0225],\n",
       "           [-0.0150,  0.0082, -0.0266,  ...,  0.0112,  0.0007, -0.0288],\n",
       "           [ 0.0042,  0.0121,  0.0178,  ..., -0.0171, -0.0172,  0.0250]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-2.8804e-02,  9.2716e-03,  3.0880e-02, -1.9075e-03, -2.9148e-02,\n",
       "            1.5161e-03, -1.6314e-02, -1.2834e-02,  2.6948e-02, -2.5903e-02,\n",
       "           -3.7491e-03, -2.8486e-03,  8.6375e-03, -4.5993e-03, -2.6027e-02,\n",
       "           -2.3151e-02,  1.9023e-02,  2.8956e-02,  2.5606e-02, -7.8710e-03,\n",
       "            2.1047e-02,  1.8598e-02, -2.4842e-02, -2.1742e-02,  2.3683e-02,\n",
       "           -1.6587e-02, -1.8220e-02, -7.4297e-03, -7.9439e-03, -2.3919e-02,\n",
       "           -2.3253e-02,  1.3756e-03, -2.7531e-02,  1.3949e-02,  1.9687e-02,\n",
       "           -1.6631e-02, -3.4857e-03,  6.6943e-03,  2.9987e-02,  1.9204e-02,\n",
       "            8.0450e-03, -3.0929e-02,  1.5685e-02,  4.0696e-03, -2.2232e-02,\n",
       "           -8.1189e-04,  9.7563e-03,  2.7532e-02, -2.8239e-02, -2.6158e-02,\n",
       "           -2.3858e-02,  3.1009e-02,  3.1245e-03,  1.9524e-02, -2.3481e-02,\n",
       "            8.1298e-03, -1.6053e-02, -2.1363e-02,  3.8721e-03, -1.7074e-02,\n",
       "            8.6320e-03,  1.8624e-02, -1.5908e-02,  1.6453e-02,  2.4279e-02,\n",
       "           -1.3049e-02, -9.0858e-03, -2.9329e-02,  2.8574e-02,  2.4069e-02,\n",
       "            7.0423e-05, -7.7661e-03, -7.7476e-03,  1.6330e-02,  1.6172e-02,\n",
       "           -2.3324e-02, -1.3125e-02, -2.5566e-02,  7.9057e-03,  1.5658e-02,\n",
       "            1.3872e-02, -2.9698e-02, -1.1206e-02,  2.7797e-02,  5.9661e-03,\n",
       "            7.9648e-04, -2.8240e-02, -2.7155e-02, -1.6194e-04, -2.7380e-02,\n",
       "            7.4104e-03,  1.3020e-02, -7.7887e-03, -1.5883e-02, -7.9109e-03,\n",
       "            2.8678e-02, -2.0027e-02, -1.3283e-03, -2.5661e-02,  7.9614e-03],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 0.0974,  0.0522,  0.0275,  0.0682, -0.0053,  0.0745,  0.0289,  0.0543,\n",
       "             0.0348,  0.0102, -0.0209, -0.0687, -0.0294,  0.0344, -0.0888, -0.0156,\n",
       "             0.0076, -0.0319,  0.0720,  0.0329,  0.0395, -0.0549,  0.0405,  0.0508,\n",
       "            -0.0750,  0.0867,  0.0195, -0.0167, -0.0938, -0.0511,  0.0739, -0.0723,\n",
       "            -0.0727, -0.0377,  0.0555, -0.0586, -0.0274, -0.0750, -0.0656,  0.0594,\n",
       "            -0.0497, -0.0356,  0.0867,  0.0598, -0.0836,  0.0244, -0.0074, -0.0482,\n",
       "            -0.0544, -0.0618, -0.0870, -0.0996, -0.0087, -0.0383,  0.0302, -0.0223,\n",
       "             0.0416, -0.0166, -0.0956,  0.0432, -0.0740,  0.0682, -0.0987, -0.0156,\n",
       "             0.0972, -0.0596, -0.0782, -0.0821,  0.0172,  0.0849,  0.0917, -0.0236,\n",
       "             0.0525,  0.0268, -0.0641,  0.0079,  0.0054,  0.0319,  0.0621,  0.0245,\n",
       "            -0.0554,  0.0460,  0.0970, -0.0517, -0.0725,  0.0018,  0.0004,  0.0287,\n",
       "             0.0034,  0.0635, -0.0492, -0.0590, -0.0439, -0.0874,  0.0593,  0.0383,\n",
       "            -0.0771,  0.0453,  0.0686, -0.0667],\n",
       "           [ 0.0889,  0.0020, -0.0412,  0.0038, -0.0808, -0.0230, -0.0073,  0.0701,\n",
       "            -0.0252, -0.0033,  0.0582,  0.0605, -0.0808,  0.0573,  0.0887, -0.0928,\n",
       "            -0.0478, -0.0237, -0.0121, -0.0296, -0.0997,  0.0971,  0.0988,  0.0054,\n",
       "            -0.0430, -0.0843, -0.0298,  0.0792, -0.0538,  0.0626, -0.0675,  0.0006,\n",
       "             0.0883, -0.0230,  0.0100,  0.0555, -0.0763,  0.0495, -0.0345,  0.0832,\n",
       "            -0.0601, -0.0326,  0.0606, -0.0767, -0.0917, -0.0788, -0.0352, -0.0663,\n",
       "            -0.0676, -0.0405, -0.0456,  0.0540, -0.0830,  0.0904,  0.0454,  0.0009,\n",
       "             0.0239, -0.0620, -0.0803,  0.0756, -0.0313,  0.0334, -0.0598, -0.0368,\n",
       "             0.0934,  0.0586,  0.0661, -0.0855, -0.0572,  0.0883,  0.0672, -0.0806,\n",
       "             0.0334, -0.0325, -0.0034,  0.0537,  0.0303, -0.0223,  0.0832,  0.0300,\n",
       "             0.0698, -0.0396, -0.0842, -0.0967, -0.0847,  0.0408,  0.0473, -0.0155,\n",
       "            -0.0975,  0.0349,  0.0662,  0.0833, -0.0895,  0.0871, -0.0108, -0.0145,\n",
       "            -0.0938,  0.0824,  0.0203,  0.0925],\n",
       "           [ 0.0473, -0.0242, -0.0550, -0.0146,  0.0214, -0.0727,  0.0951, -0.0857,\n",
       "             0.0674,  0.0055,  0.0936, -0.0262, -0.0444,  0.0880,  0.0968,  0.0034,\n",
       "            -0.0648, -0.0535, -0.0531, -0.0796, -0.0257,  0.0712,  0.0760,  0.0929,\n",
       "             0.0753,  0.0497,  0.0408, -0.0779, -0.0963,  0.0501,  0.0766, -0.0086,\n",
       "            -0.0880, -0.0609,  0.0310,  0.0613, -0.0031, -0.0226, -0.0943,  0.0205,\n",
       "            -0.0381,  0.0961, -0.0291, -0.0123,  0.0552, -0.0660, -0.0793,  0.0548,\n",
       "            -0.0275, -0.0623,  0.0094,  0.0929,  0.0331,  0.0752,  0.0180,  0.0248,\n",
       "            -0.0263, -0.0946, -0.0800, -0.0255,  0.0406, -0.0452, -0.0078,  0.0248,\n",
       "            -0.0859, -0.0421,  0.0090,  0.0339, -0.0335,  0.0532,  0.0427, -0.0092,\n",
       "             0.0296, -0.0416, -0.0356, -0.0435, -0.0743,  0.0483,  0.0271,  0.0325,\n",
       "            -0.0989,  0.0349,  0.0370,  0.0242,  0.0882,  0.0287,  0.0384,  0.0595,\n",
       "            -0.0796,  0.0640, -0.0507,  0.0499, -0.0909, -0.0983, -0.0195, -0.0323,\n",
       "             0.0226,  0.0360, -0.0469, -0.0394],\n",
       "           [ 0.0779,  0.0374, -0.0632,  0.0808,  0.0424, -0.0597, -0.0364,  0.0034,\n",
       "            -0.0123,  0.0446,  0.0670, -0.0925,  0.0994, -0.0527,  0.0704,  0.0546,\n",
       "            -0.0623,  0.0533, -0.0286, -0.0594, -0.0485, -0.0645,  0.0441, -0.0473,\n",
       "            -0.0998, -0.0182,  0.0031,  0.0006,  0.0011,  0.0858, -0.0484,  0.0131,\n",
       "            -0.0628, -0.0600,  0.0426,  0.0812, -0.0655,  0.0619,  0.0690,  0.0066,\n",
       "            -0.0647,  0.0779, -0.0118, -0.0926,  0.0548,  0.0430, -0.0965, -0.0512,\n",
       "             0.0440,  0.0713,  0.0556,  0.0646, -0.0008, -0.0313,  0.0377, -0.0116,\n",
       "             0.0590, -0.0221,  0.0379, -0.0585,  0.0454,  0.0624,  0.0052, -0.0739,\n",
       "             0.0127,  0.0755, -0.0459, -0.0027, -0.0121, -0.0381, -0.0948,  0.0947,\n",
       "             0.0695,  0.0121, -0.0010, -0.0427, -0.0960,  0.0438, -0.0662, -0.0190,\n",
       "            -0.0249,  0.0243,  0.0472, -0.0715,  0.0810,  0.0925, -0.0258, -0.0303,\n",
       "             0.0628, -0.0313, -0.0372,  0.0187, -0.0191, -0.0744,  0.0318, -0.0496,\n",
       "             0.0910, -0.0388, -0.0535,  0.0603],\n",
       "           [-0.0701, -0.0458,  0.0187,  0.0414,  0.0030, -0.0937,  0.0032,  0.0172,\n",
       "             0.0652,  0.0269, -0.0268, -0.0458, -0.0128, -0.0433,  0.0409, -0.0301,\n",
       "             0.0709,  0.0791, -0.0411,  0.0136,  0.0873,  0.0376,  0.0188, -0.0698,\n",
       "             0.0700, -0.0202,  0.0902, -0.0395, -0.0328,  0.0977,  0.0353,  0.0336,\n",
       "            -0.0559, -0.0956, -0.0309,  0.0836,  0.0804, -0.0082, -0.0443, -0.0898,\n",
       "            -0.0967,  0.0005, -0.0675, -0.0373,  0.0699, -0.0976, -0.0321,  0.0671,\n",
       "             0.0495,  0.0407,  0.0374, -0.0939,  0.0356, -0.0588,  0.0776, -0.0201,\n",
       "             0.0483, -0.0084, -0.0084, -0.0159, -0.0068,  0.0207,  0.0784,  0.0013,\n",
       "            -0.0157, -0.0656,  0.0437, -0.0608, -0.0624, -0.0565,  0.0003,  0.0681,\n",
       "            -0.0444,  0.0159, -0.0945, -0.0637, -0.0828,  0.0299, -0.0711, -0.0235,\n",
       "             0.0291, -0.0527, -0.0776,  0.0897, -0.0524,  0.0216, -0.0271, -0.0661,\n",
       "            -0.0188,  0.0540,  0.0926,  0.0779,  0.0778, -0.0614,  0.0995, -0.0148,\n",
       "             0.0427, -0.0559, -0.0195, -0.0265],\n",
       "           [ 0.0650, -0.0511,  0.0294, -0.0917,  0.0963,  0.0127,  0.0867, -0.0706,\n",
       "            -0.0877, -0.0551, -0.0093,  0.0823, -0.0586,  0.0799,  0.0196, -0.0808,\n",
       "            -0.0118, -0.0889, -0.0951,  0.0621, -0.0141, -0.0660, -0.0205, -0.0458,\n",
       "            -0.0368, -0.0069,  0.0664,  0.0199, -0.0412, -0.0213, -0.0104, -0.0789,\n",
       "             0.0082, -0.0326,  0.0054, -0.0873, -0.0157, -0.0412, -0.0733,  0.0906,\n",
       "            -0.0335,  0.0779, -0.0072,  0.0262, -0.0705,  0.0573,  0.0851,  0.0971,\n",
       "             0.0428,  0.0387, -0.0225, -0.0110, -0.0832, -0.0652, -0.0669,  0.0675,\n",
       "             0.0299,  0.0961,  0.0982,  0.0951, -0.0583,  0.0957,  0.0904,  0.0728,\n",
       "             0.0212,  0.0582,  0.0320, -0.0876, -0.0893, -0.0581,  0.0937,  0.0350,\n",
       "             0.0848, -0.0523,  0.0130,  0.0290,  0.0657, -0.0973, -0.0346,  0.0974,\n",
       "            -0.0759, -0.0244,  0.0398, -0.0364,  0.0049, -0.0991, -0.0620,  0.0599,\n",
       "             0.0872,  0.0793, -0.0498,  0.0779, -0.0882, -0.0761, -0.0439,  0.0250,\n",
       "            -0.0805, -0.0922, -0.0021,  0.0485],\n",
       "           [-0.0568, -0.0036,  0.0745, -0.0443,  0.0116, -0.0050,  0.0453, -0.0876,\n",
       "             0.0458,  0.0676, -0.0612,  0.0836, -0.0560, -0.0584,  0.0277, -0.0288,\n",
       "            -0.0506, -0.0207, -0.0897,  0.0197,  0.0047, -0.0566,  0.0560, -0.0236,\n",
       "            -0.0811, -0.0454,  0.0800, -0.0338, -0.0684,  0.0540,  0.0593,  0.0022,\n",
       "            -0.0038, -0.0116,  0.0470, -0.0836, -0.0424,  0.0060,  0.0804, -0.0216,\n",
       "            -0.0518, -0.0586, -0.0343,  0.0372, -0.0307, -0.0958, -0.0142, -0.0225,\n",
       "            -0.0924,  0.0816,  0.0083,  0.0100,  0.0764, -0.0409, -0.0924,  0.0559,\n",
       "            -0.0251, -0.0702, -0.0110,  0.0780, -0.0325, -0.0150, -0.0598, -0.0499,\n",
       "            -0.0749, -0.0522,  0.0823, -0.0469, -0.0973, -0.0760, -0.0301,  0.0409,\n",
       "            -0.0951, -0.0083,  0.0220,  0.0313, -0.0876,  0.0835, -0.0191, -0.0357,\n",
       "             0.0209,  0.0357,  0.0504, -0.0355, -0.0032, -0.0198,  0.0468,  0.0901,\n",
       "             0.0168, -0.0398, -0.0381, -0.0030,  0.0627,  0.0355,  0.0743,  0.0076,\n",
       "             0.0680,  0.0890, -0.0935,  0.0148],\n",
       "           [-0.0289, -0.0696, -0.0513,  0.0235,  0.0446, -0.0840,  0.0618, -0.0870,\n",
       "            -0.0784,  0.0407, -0.0972,  0.0312, -0.0460,  0.0979, -0.0598, -0.0166,\n",
       "            -0.0110, -0.0248, -0.0606, -0.0374, -0.0679,  0.0278,  0.0148, -0.0470,\n",
       "             0.0236,  0.0907, -0.0708, -0.0568, -0.0343, -0.0765, -0.0361, -0.0919,\n",
       "             0.0582, -0.0452,  0.0500, -0.0098, -0.0155, -0.0791, -0.0424,  0.0228,\n",
       "             0.0021, -0.0879, -0.0874, -0.0942, -0.0779,  0.0199, -0.0226,  0.0468,\n",
       "            -0.0270, -0.0188, -0.0704,  0.0444, -0.0842,  0.0174,  0.0826,  0.0662,\n",
       "             0.0089, -0.0418, -0.0366, -0.0243,  0.0001, -0.0561,  0.0759, -0.0327,\n",
       "             0.0963, -0.0600, -0.0849,  0.0247, -0.0801, -0.0028, -0.0467, -0.0112,\n",
       "            -0.0940, -0.0795,  0.0738, -0.0412, -0.0801, -0.0708,  0.0300,  0.0701,\n",
       "            -0.0024,  0.0303, -0.0167,  0.0287, -0.0795, -0.0402,  0.0520,  0.0178,\n",
       "            -0.0603, -0.0773,  0.0518,  0.0733,  0.0503,  0.0513,  0.0577, -0.0527,\n",
       "             0.0613, -0.0687,  0.0367, -0.0874],\n",
       "           [-0.0112,  0.0576,  0.0439, -0.0294, -0.0744, -0.0515,  0.0499,  0.0542,\n",
       "             0.0664, -0.0493,  0.0444, -0.0437,  0.0028,  0.0513, -0.0736,  0.0466,\n",
       "            -0.0426, -0.0665, -0.0692, -0.0797, -0.0559, -0.0496, -0.0256, -0.0763,\n",
       "            -0.0026,  0.0798, -0.0583, -0.0215, -0.0898,  0.0543, -0.0586, -0.0379,\n",
       "            -0.0107,  0.0274,  0.0944, -0.0249, -0.0064,  0.0488,  0.0396,  0.0987,\n",
       "             0.0285, -0.0009,  0.0352,  0.0265,  0.0914,  0.0313, -0.0530,  0.0431,\n",
       "            -0.0507, -0.0619,  0.0165,  0.0928, -0.0689, -0.0345,  0.0238, -0.0102,\n",
       "             0.0277,  0.0404, -0.0232,  0.0568,  0.0486,  0.0056,  0.0525,  0.0322,\n",
       "             0.0739, -0.0168, -0.0693, -0.0578, -0.0349,  0.0271, -0.0405,  0.0504,\n",
       "             0.0693, -0.0518, -0.0095,  0.0657,  0.0127,  0.0321,  0.0971, -0.0109,\n",
       "             0.0192,  0.0365, -0.0739, -0.0906, -0.0790, -0.0845,  0.0538, -0.0416,\n",
       "            -0.0690,  0.0441,  0.0969, -0.0815, -0.0061, -0.0764, -0.0210,  0.0627,\n",
       "             0.0117, -0.0096, -0.0771, -0.0054],\n",
       "           [-0.0867, -0.0218,  0.0834, -0.0067,  0.0481,  0.0061,  0.0954, -0.0634,\n",
       "            -0.0399,  0.0191,  0.0636, -0.0605, -0.0528, -0.0851, -0.0058, -0.0065,\n",
       "             0.0682, -0.0234, -0.0842,  0.0178,  0.0091, -0.0781,  0.0790, -0.0206,\n",
       "            -0.0113,  0.0577, -0.0868,  0.0296, -0.0904, -0.0846,  0.0461,  0.0626,\n",
       "            -0.0812, -0.0590,  0.0633,  0.0199, -0.0721,  0.0197,  0.0373,  0.0145,\n",
       "            -0.0830, -0.0540, -0.0830,  0.0537,  0.0151,  0.0675, -0.0217,  0.0003,\n",
       "             0.0354,  0.0053,  0.0157,  0.0922, -0.0477,  0.0491,  0.0059, -0.0219,\n",
       "             0.0071,  0.0835, -0.0329, -0.0912,  0.0065,  0.0438,  0.0046, -0.0393,\n",
       "            -0.0338, -0.0608,  0.0990, -0.0339,  0.0267, -0.0561,  0.0420,  0.0109,\n",
       "             0.0379, -0.0111, -0.0025, -0.0175, -0.0685,  0.0667,  0.0967,  0.0597,\n",
       "            -0.0975,  0.0516, -0.0394, -0.0228,  0.0332,  0.0347,  0.0815,  0.0722,\n",
       "            -0.0335, -0.0142,  0.0587,  0.0743,  0.0332, -0.0773,  0.0525, -0.0530,\n",
       "            -0.0890, -0.0846,  0.0326, -0.0356]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.0570,  0.0820, -0.0259, -0.0420, -0.0699, -0.0519,  0.0199, -0.0958,\n",
       "           -0.0348, -0.0131], requires_grad=True)],\n",
       "  'lr': 0.0001,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = optimizer.param_groups[0]\n",
    "type(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0132, -0.0153, -0.0047,  ...,  0.0122, -0.0004,  0.0304],\n",
       "         [-0.0260, -0.0314, -0.0120,  ...,  0.0271,  0.0107, -0.0053],\n",
       "         [-0.0219, -0.0167,  0.0007,  ..., -0.0293,  0.0283,  0.0269],\n",
       "         ...,\n",
       "         [-0.0116, -0.0082,  0.0290,  ..., -0.0251, -0.0084, -0.0225],\n",
       "         [-0.0150,  0.0082, -0.0266,  ...,  0.0112,  0.0007, -0.0288],\n",
       "         [ 0.0042,  0.0121,  0.0178,  ..., -0.0171, -0.0172,  0.0250]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-2.8804e-02,  9.2716e-03,  3.0880e-02, -1.9075e-03, -2.9148e-02,\n",
       "          1.5161e-03, -1.6314e-02, -1.2834e-02,  2.6948e-02, -2.5903e-02,\n",
       "         -3.7491e-03, -2.8486e-03,  8.6375e-03, -4.5993e-03, -2.6027e-02,\n",
       "         -2.3151e-02,  1.9023e-02,  2.8956e-02,  2.5606e-02, -7.8710e-03,\n",
       "          2.1047e-02,  1.8598e-02, -2.4842e-02, -2.1742e-02,  2.3683e-02,\n",
       "         -1.6587e-02, -1.8220e-02, -7.4297e-03, -7.9439e-03, -2.3919e-02,\n",
       "         -2.3253e-02,  1.3756e-03, -2.7531e-02,  1.3949e-02,  1.9687e-02,\n",
       "         -1.6631e-02, -3.4857e-03,  6.6943e-03,  2.9987e-02,  1.9204e-02,\n",
       "          8.0450e-03, -3.0929e-02,  1.5685e-02,  4.0696e-03, -2.2232e-02,\n",
       "         -8.1189e-04,  9.7563e-03,  2.7532e-02, -2.8239e-02, -2.6158e-02,\n",
       "         -2.3858e-02,  3.1009e-02,  3.1245e-03,  1.9524e-02, -2.3481e-02,\n",
       "          8.1298e-03, -1.6053e-02, -2.1363e-02,  3.8721e-03, -1.7074e-02,\n",
       "          8.6320e-03,  1.8624e-02, -1.5908e-02,  1.6453e-02,  2.4279e-02,\n",
       "         -1.3049e-02, -9.0858e-03, -2.9329e-02,  2.8574e-02,  2.4069e-02,\n",
       "          7.0423e-05, -7.7661e-03, -7.7476e-03,  1.6330e-02,  1.6172e-02,\n",
       "         -2.3324e-02, -1.3125e-02, -2.5566e-02,  7.9057e-03,  1.5658e-02,\n",
       "          1.3872e-02, -2.9698e-02, -1.1206e-02,  2.7797e-02,  5.9661e-03,\n",
       "          7.9648e-04, -2.8240e-02, -2.7155e-02, -1.6194e-04, -2.7380e-02,\n",
       "          7.4104e-03,  1.3020e-02, -7.7887e-03, -1.5883e-02, -7.9109e-03,\n",
       "          2.8678e-02, -2.0027e-02, -1.3283e-03, -2.5661e-02,  7.9614e-03],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0974,  0.0522,  0.0275,  0.0682, -0.0053,  0.0745,  0.0289,  0.0543,\n",
       "           0.0348,  0.0102, -0.0209, -0.0687, -0.0294,  0.0344, -0.0888, -0.0156,\n",
       "           0.0076, -0.0319,  0.0720,  0.0329,  0.0395, -0.0549,  0.0405,  0.0508,\n",
       "          -0.0750,  0.0867,  0.0195, -0.0167, -0.0938, -0.0511,  0.0739, -0.0723,\n",
       "          -0.0727, -0.0377,  0.0555, -0.0586, -0.0274, -0.0750, -0.0656,  0.0594,\n",
       "          -0.0497, -0.0356,  0.0867,  0.0598, -0.0836,  0.0244, -0.0074, -0.0482,\n",
       "          -0.0544, -0.0618, -0.0870, -0.0996, -0.0087, -0.0383,  0.0302, -0.0223,\n",
       "           0.0416, -0.0166, -0.0956,  0.0432, -0.0740,  0.0682, -0.0987, -0.0156,\n",
       "           0.0972, -0.0596, -0.0782, -0.0821,  0.0172,  0.0849,  0.0917, -0.0236,\n",
       "           0.0525,  0.0268, -0.0641,  0.0079,  0.0054,  0.0319,  0.0621,  0.0245,\n",
       "          -0.0554,  0.0460,  0.0970, -0.0517, -0.0725,  0.0018,  0.0004,  0.0287,\n",
       "           0.0034,  0.0635, -0.0492, -0.0590, -0.0439, -0.0874,  0.0593,  0.0383,\n",
       "          -0.0771,  0.0453,  0.0686, -0.0667],\n",
       "         [ 0.0889,  0.0020, -0.0412,  0.0038, -0.0808, -0.0230, -0.0073,  0.0701,\n",
       "          -0.0252, -0.0033,  0.0582,  0.0605, -0.0808,  0.0573,  0.0887, -0.0928,\n",
       "          -0.0478, -0.0237, -0.0121, -0.0296, -0.0997,  0.0971,  0.0988,  0.0054,\n",
       "          -0.0430, -0.0843, -0.0298,  0.0792, -0.0538,  0.0626, -0.0675,  0.0006,\n",
       "           0.0883, -0.0230,  0.0100,  0.0555, -0.0763,  0.0495, -0.0345,  0.0832,\n",
       "          -0.0601, -0.0326,  0.0606, -0.0767, -0.0917, -0.0788, -0.0352, -0.0663,\n",
       "          -0.0676, -0.0405, -0.0456,  0.0540, -0.0830,  0.0904,  0.0454,  0.0009,\n",
       "           0.0239, -0.0620, -0.0803,  0.0756, -0.0313,  0.0334, -0.0598, -0.0368,\n",
       "           0.0934,  0.0586,  0.0661, -0.0855, -0.0572,  0.0883,  0.0672, -0.0806,\n",
       "           0.0334, -0.0325, -0.0034,  0.0537,  0.0303, -0.0223,  0.0832,  0.0300,\n",
       "           0.0698, -0.0396, -0.0842, -0.0967, -0.0847,  0.0408,  0.0473, -0.0155,\n",
       "          -0.0975,  0.0349,  0.0662,  0.0833, -0.0895,  0.0871, -0.0108, -0.0145,\n",
       "          -0.0938,  0.0824,  0.0203,  0.0925],\n",
       "         [ 0.0473, -0.0242, -0.0550, -0.0146,  0.0214, -0.0727,  0.0951, -0.0857,\n",
       "           0.0674,  0.0055,  0.0936, -0.0262, -0.0444,  0.0880,  0.0968,  0.0034,\n",
       "          -0.0648, -0.0535, -0.0531, -0.0796, -0.0257,  0.0712,  0.0760,  0.0929,\n",
       "           0.0753,  0.0497,  0.0408, -0.0779, -0.0963,  0.0501,  0.0766, -0.0086,\n",
       "          -0.0880, -0.0609,  0.0310,  0.0613, -0.0031, -0.0226, -0.0943,  0.0205,\n",
       "          -0.0381,  0.0961, -0.0291, -0.0123,  0.0552, -0.0660, -0.0793,  0.0548,\n",
       "          -0.0275, -0.0623,  0.0094,  0.0929,  0.0331,  0.0752,  0.0180,  0.0248,\n",
       "          -0.0263, -0.0946, -0.0800, -0.0255,  0.0406, -0.0452, -0.0078,  0.0248,\n",
       "          -0.0859, -0.0421,  0.0090,  0.0339, -0.0335,  0.0532,  0.0427, -0.0092,\n",
       "           0.0296, -0.0416, -0.0356, -0.0435, -0.0743,  0.0483,  0.0271,  0.0325,\n",
       "          -0.0989,  0.0349,  0.0370,  0.0242,  0.0882,  0.0287,  0.0384,  0.0595,\n",
       "          -0.0796,  0.0640, -0.0507,  0.0499, -0.0909, -0.0983, -0.0195, -0.0323,\n",
       "           0.0226,  0.0360, -0.0469, -0.0394],\n",
       "         [ 0.0779,  0.0374, -0.0632,  0.0808,  0.0424, -0.0597, -0.0364,  0.0034,\n",
       "          -0.0123,  0.0446,  0.0670, -0.0925,  0.0994, -0.0527,  0.0704,  0.0546,\n",
       "          -0.0623,  0.0533, -0.0286, -0.0594, -0.0485, -0.0645,  0.0441, -0.0473,\n",
       "          -0.0998, -0.0182,  0.0031,  0.0006,  0.0011,  0.0858, -0.0484,  0.0131,\n",
       "          -0.0628, -0.0600,  0.0426,  0.0812, -0.0655,  0.0619,  0.0690,  0.0066,\n",
       "          -0.0647,  0.0779, -0.0118, -0.0926,  0.0548,  0.0430, -0.0965, -0.0512,\n",
       "           0.0440,  0.0713,  0.0556,  0.0646, -0.0008, -0.0313,  0.0377, -0.0116,\n",
       "           0.0590, -0.0221,  0.0379, -0.0585,  0.0454,  0.0624,  0.0052, -0.0739,\n",
       "           0.0127,  0.0755, -0.0459, -0.0027, -0.0121, -0.0381, -0.0948,  0.0947,\n",
       "           0.0695,  0.0121, -0.0010, -0.0427, -0.0960,  0.0438, -0.0662, -0.0190,\n",
       "          -0.0249,  0.0243,  0.0472, -0.0715,  0.0810,  0.0925, -0.0258, -0.0303,\n",
       "           0.0628, -0.0313, -0.0372,  0.0187, -0.0191, -0.0744,  0.0318, -0.0496,\n",
       "           0.0910, -0.0388, -0.0535,  0.0603],\n",
       "         [-0.0701, -0.0458,  0.0187,  0.0414,  0.0030, -0.0937,  0.0032,  0.0172,\n",
       "           0.0652,  0.0269, -0.0268, -0.0458, -0.0128, -0.0433,  0.0409, -0.0301,\n",
       "           0.0709,  0.0791, -0.0411,  0.0136,  0.0873,  0.0376,  0.0188, -0.0698,\n",
       "           0.0700, -0.0202,  0.0902, -0.0395, -0.0328,  0.0977,  0.0353,  0.0336,\n",
       "          -0.0559, -0.0956, -0.0309,  0.0836,  0.0804, -0.0082, -0.0443, -0.0898,\n",
       "          -0.0967,  0.0005, -0.0675, -0.0373,  0.0699, -0.0976, -0.0321,  0.0671,\n",
       "           0.0495,  0.0407,  0.0374, -0.0939,  0.0356, -0.0588,  0.0776, -0.0201,\n",
       "           0.0483, -0.0084, -0.0084, -0.0159, -0.0068,  0.0207,  0.0784,  0.0013,\n",
       "          -0.0157, -0.0656,  0.0437, -0.0608, -0.0624, -0.0565,  0.0003,  0.0681,\n",
       "          -0.0444,  0.0159, -0.0945, -0.0637, -0.0828,  0.0299, -0.0711, -0.0235,\n",
       "           0.0291, -0.0527, -0.0776,  0.0897, -0.0524,  0.0216, -0.0271, -0.0661,\n",
       "          -0.0188,  0.0540,  0.0926,  0.0779,  0.0778, -0.0614,  0.0995, -0.0148,\n",
       "           0.0427, -0.0559, -0.0195, -0.0265],\n",
       "         [ 0.0650, -0.0511,  0.0294, -0.0917,  0.0963,  0.0127,  0.0867, -0.0706,\n",
       "          -0.0877, -0.0551, -0.0093,  0.0823, -0.0586,  0.0799,  0.0196, -0.0808,\n",
       "          -0.0118, -0.0889, -0.0951,  0.0621, -0.0141, -0.0660, -0.0205, -0.0458,\n",
       "          -0.0368, -0.0069,  0.0664,  0.0199, -0.0412, -0.0213, -0.0104, -0.0789,\n",
       "           0.0082, -0.0326,  0.0054, -0.0873, -0.0157, -0.0412, -0.0733,  0.0906,\n",
       "          -0.0335,  0.0779, -0.0072,  0.0262, -0.0705,  0.0573,  0.0851,  0.0971,\n",
       "           0.0428,  0.0387, -0.0225, -0.0110, -0.0832, -0.0652, -0.0669,  0.0675,\n",
       "           0.0299,  0.0961,  0.0982,  0.0951, -0.0583,  0.0957,  0.0904,  0.0728,\n",
       "           0.0212,  0.0582,  0.0320, -0.0876, -0.0893, -0.0581,  0.0937,  0.0350,\n",
       "           0.0848, -0.0523,  0.0130,  0.0290,  0.0657, -0.0973, -0.0346,  0.0974,\n",
       "          -0.0759, -0.0244,  0.0398, -0.0364,  0.0049, -0.0991, -0.0620,  0.0599,\n",
       "           0.0872,  0.0793, -0.0498,  0.0779, -0.0882, -0.0761, -0.0439,  0.0250,\n",
       "          -0.0805, -0.0922, -0.0021,  0.0485],\n",
       "         [-0.0568, -0.0036,  0.0745, -0.0443,  0.0116, -0.0050,  0.0453, -0.0876,\n",
       "           0.0458,  0.0676, -0.0612,  0.0836, -0.0560, -0.0584,  0.0277, -0.0288,\n",
       "          -0.0506, -0.0207, -0.0897,  0.0197,  0.0047, -0.0566,  0.0560, -0.0236,\n",
       "          -0.0811, -0.0454,  0.0800, -0.0338, -0.0684,  0.0540,  0.0593,  0.0022,\n",
       "          -0.0038, -0.0116,  0.0470, -0.0836, -0.0424,  0.0060,  0.0804, -0.0216,\n",
       "          -0.0518, -0.0586, -0.0343,  0.0372, -0.0307, -0.0958, -0.0142, -0.0225,\n",
       "          -0.0924,  0.0816,  0.0083,  0.0100,  0.0764, -0.0409, -0.0924,  0.0559,\n",
       "          -0.0251, -0.0702, -0.0110,  0.0780, -0.0325, -0.0150, -0.0598, -0.0499,\n",
       "          -0.0749, -0.0522,  0.0823, -0.0469, -0.0973, -0.0760, -0.0301,  0.0409,\n",
       "          -0.0951, -0.0083,  0.0220,  0.0313, -0.0876,  0.0835, -0.0191, -0.0357,\n",
       "           0.0209,  0.0357,  0.0504, -0.0355, -0.0032, -0.0198,  0.0468,  0.0901,\n",
       "           0.0168, -0.0398, -0.0381, -0.0030,  0.0627,  0.0355,  0.0743,  0.0076,\n",
       "           0.0680,  0.0890, -0.0935,  0.0148],\n",
       "         [-0.0289, -0.0696, -0.0513,  0.0235,  0.0446, -0.0840,  0.0618, -0.0870,\n",
       "          -0.0784,  0.0407, -0.0972,  0.0312, -0.0460,  0.0979, -0.0598, -0.0166,\n",
       "          -0.0110, -0.0248, -0.0606, -0.0374, -0.0679,  0.0278,  0.0148, -0.0470,\n",
       "           0.0236,  0.0907, -0.0708, -0.0568, -0.0343, -0.0765, -0.0361, -0.0919,\n",
       "           0.0582, -0.0452,  0.0500, -0.0098, -0.0155, -0.0791, -0.0424,  0.0228,\n",
       "           0.0021, -0.0879, -0.0874, -0.0942, -0.0779,  0.0199, -0.0226,  0.0468,\n",
       "          -0.0270, -0.0188, -0.0704,  0.0444, -0.0842,  0.0174,  0.0826,  0.0662,\n",
       "           0.0089, -0.0418, -0.0366, -0.0243,  0.0001, -0.0561,  0.0759, -0.0327,\n",
       "           0.0963, -0.0600, -0.0849,  0.0247, -0.0801, -0.0028, -0.0467, -0.0112,\n",
       "          -0.0940, -0.0795,  0.0738, -0.0412, -0.0801, -0.0708,  0.0300,  0.0701,\n",
       "          -0.0024,  0.0303, -0.0167,  0.0287, -0.0795, -0.0402,  0.0520,  0.0178,\n",
       "          -0.0603, -0.0773,  0.0518,  0.0733,  0.0503,  0.0513,  0.0577, -0.0527,\n",
       "           0.0613, -0.0687,  0.0367, -0.0874],\n",
       "         [-0.0112,  0.0576,  0.0439, -0.0294, -0.0744, -0.0515,  0.0499,  0.0542,\n",
       "           0.0664, -0.0493,  0.0444, -0.0437,  0.0028,  0.0513, -0.0736,  0.0466,\n",
       "          -0.0426, -0.0665, -0.0692, -0.0797, -0.0559, -0.0496, -0.0256, -0.0763,\n",
       "          -0.0026,  0.0798, -0.0583, -0.0215, -0.0898,  0.0543, -0.0586, -0.0379,\n",
       "          -0.0107,  0.0274,  0.0944, -0.0249, -0.0064,  0.0488,  0.0396,  0.0987,\n",
       "           0.0285, -0.0009,  0.0352,  0.0265,  0.0914,  0.0313, -0.0530,  0.0431,\n",
       "          -0.0507, -0.0619,  0.0165,  0.0928, -0.0689, -0.0345,  0.0238, -0.0102,\n",
       "           0.0277,  0.0404, -0.0232,  0.0568,  0.0486,  0.0056,  0.0525,  0.0322,\n",
       "           0.0739, -0.0168, -0.0693, -0.0578, -0.0349,  0.0271, -0.0405,  0.0504,\n",
       "           0.0693, -0.0518, -0.0095,  0.0657,  0.0127,  0.0321,  0.0971, -0.0109,\n",
       "           0.0192,  0.0365, -0.0739, -0.0906, -0.0790, -0.0845,  0.0538, -0.0416,\n",
       "          -0.0690,  0.0441,  0.0969, -0.0815, -0.0061, -0.0764, -0.0210,  0.0627,\n",
       "           0.0117, -0.0096, -0.0771, -0.0054],\n",
       "         [-0.0867, -0.0218,  0.0834, -0.0067,  0.0481,  0.0061,  0.0954, -0.0634,\n",
       "          -0.0399,  0.0191,  0.0636, -0.0605, -0.0528, -0.0851, -0.0058, -0.0065,\n",
       "           0.0682, -0.0234, -0.0842,  0.0178,  0.0091, -0.0781,  0.0790, -0.0206,\n",
       "          -0.0113,  0.0577, -0.0868,  0.0296, -0.0904, -0.0846,  0.0461,  0.0626,\n",
       "          -0.0812, -0.0590,  0.0633,  0.0199, -0.0721,  0.0197,  0.0373,  0.0145,\n",
       "          -0.0830, -0.0540, -0.0830,  0.0537,  0.0151,  0.0675, -0.0217,  0.0003,\n",
       "           0.0354,  0.0053,  0.0157,  0.0922, -0.0477,  0.0491,  0.0059, -0.0219,\n",
       "           0.0071,  0.0835, -0.0329, -0.0912,  0.0065,  0.0438,  0.0046, -0.0393,\n",
       "          -0.0338, -0.0608,  0.0990, -0.0339,  0.0267, -0.0561,  0.0420,  0.0109,\n",
       "           0.0379, -0.0111, -0.0025, -0.0175, -0.0685,  0.0667,  0.0967,  0.0597,\n",
       "          -0.0975,  0.0516, -0.0394, -0.0228,  0.0332,  0.0347,  0.0815,  0.0722,\n",
       "          -0.0335, -0.0142,  0.0587,  0.0743,  0.0332, -0.0773,  0.0525, -0.0530,\n",
       "          -0.0890, -0.0846,  0.0326, -0.0356]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0570,  0.0820, -0.0259, -0.0420, -0.0699, -0.0519,  0.0199, -0.0958,\n",
       "         -0.0348, -0.0131], requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['params'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001\n",
      "momentum: 0\n",
      "nesterov: False\n",
      "weight_decay: 0\n"
     ]
    }
   ],
   "source": [
    "for optim_param in ('lr', 'momentum', 'nesterov', 'weight_decay'):\n",
    "    print(f'{optim_param}: {p[optim_param]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have to save a DL model, we definitely **need** to save model parameters (e.g. _inference_ ), but for other cases (i.e. _model checkpoint_ ) we **also need** to save **optimiser** `parameters` and `hyper-parameters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `state_dict`\n",
    "\n",
    "A `state_dict` is an integral entity if you are interested in saving or loading models from PyTorch. \n",
    "\n",
    "Because `state_dict` objects are Python dictionaries, they can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers. \n",
    "\n",
    "Note that **only** layers with learnable parameters and registered buffers (e.g. batchnorm’s running_mean) have entries in the model’s `state_dict`. Optimizer objects (`torch.optim`) also have a `state_dict`, which contains information about the optimizer’s state, as well as the **hyperparameters** used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t torch.Size([100, 1000])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([10, 100])\n",
      "linear2.bias \t torch.Size([10])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [140227236321408, 140227235900480, 140227235899072, 140227235901184]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading models for Inference in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance of a `torch.nn.Module` can be saved using the `torch.save()` function.\n",
    "\n",
    "Saving the model’s `state_dict` with the `torch.save()` function will give you the most flexibility for restoring the model later. \n",
    "\n",
    "This is the **recommended method** for saving models, because it is only really necessary to save the trained model’s learned parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), \"model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common PyTorch **convention** is to save models using either a `.pt` or `.pth` file extension.\n",
    "\n",
    "Notice that the `load_state_dict()` function takes a dictionary object, NOT a `path` to a saved object. \n",
    "\n",
    "This means that you **must** deserialize the saved `state_dict` before you pass it to the `load_state_dict()` function. \n",
    "\n",
    "For example, you **CANNOT** just load using `model.load_state_dict(\"path_to_file.pt\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving and Loading Entire Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try the same thing with the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriele/Documents/fbk/miarad/talk/intro_deep_learning/tutorial_pytorch/.venv/lib/python3.8/site-packages/torch/serialization.py:401: UserWarning: Couldn't retrieve source code for container of type TwoLayerNet. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "# Load\n",
    "model = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight \t torch.Size([100, 1000])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([10, 100])\n",
      "linear2.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading a general `checkpoint model` for inference or resuming training can be helpful for picking up where you last left off. \n",
    "\n",
    "When saving a general checkpoint, you must save more than just the model’s `state_dict`. \n",
    "\n",
    "It is **also important** to save the **optimizer**’s `state_dict`, as this contains buffers and parameters that are updated as the model trains. \n",
    "\n",
    "**Moreover**, you might also want to save the `epoch` you left off on, the latest recorded `training loss`, external layers, and more, based on your own algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional information\n",
    "EPOCH = 5\n",
    "LOSS = 0.4\n",
    "CHKPOINT = \"model_checpoint.pth\"\n",
    "\n",
    "torch.save({'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, CHKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "checkpoint = torch.load(CHKPOINT)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t torch.Size([100, 1000])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([10, 100])\n",
      "linear2.bias \t torch.Size([10])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [140227209983552, 140227209983936, 140227209984128, 140227209984192]}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from Checkpoint:  0.4\n",
      "Epoch:  5\n"
     ]
    }
   ],
   "source": [
    "print('Loss from Checkpoint: ', loss)\n",
    "print('Epoch: ', epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
